{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Y1-kXU_4jz7L"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('mobile_price_classification.csv')"
      ],
      "metadata": {
        "id": "cL2Jew1Lj2Y9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "M6thNeHWkDlS",
        "outputId": "9332fdc0-2b5c-4042-a03b-bc44b7429695"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   battery_power  bluetooth  clock_speed  dual_sim  front_cam  4G  int_memory  \\\n",
              "0            842          0          2.2         0          1   0           7   \n",
              "1           1021          1          0.5         1          0   1          53   \n",
              "2            563          1          0.5         1          2   1          41   \n",
              "3            615          1          2.5         0          0   0          10   \n",
              "4           1821          1          1.2         0         13   1          44   \n",
              "\n",
              "   m_dep  mobile_wt  n_cores  ...  px_height  px_width   ram  sc_h  sc_w  \\\n",
              "0    0.6        188        2  ...         20       756  2549     9     7   \n",
              "1    0.7        136        3  ...        905      1988  2631    17     3   \n",
              "2    0.9        145        5  ...       1263      1716  2603    11     2   \n",
              "3    0.8        131        6  ...       1216      1786  2769    16     8   \n",
              "4    0.6        141        2  ...       1208      1212  1411     8     2   \n",
              "\n",
              "   talk_time  three_g  touch_screen  wifi  price_range  \n",
              "0         19        0             0     1            1  \n",
              "1          7        1             1     0            2  \n",
              "2          9        1             1     0            2  \n",
              "3         11        1             0     0            2  \n",
              "4         15        1             1     0            1  \n",
              "\n",
              "[5 rows x 21 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ac62f5ce-f85b-404c-a89e-7347c79483ab\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>battery_power</th>\n",
              "      <th>bluetooth</th>\n",
              "      <th>clock_speed</th>\n",
              "      <th>dual_sim</th>\n",
              "      <th>front_cam</th>\n",
              "      <th>4G</th>\n",
              "      <th>int_memory</th>\n",
              "      <th>m_dep</th>\n",
              "      <th>mobile_wt</th>\n",
              "      <th>n_cores</th>\n",
              "      <th>...</th>\n",
              "      <th>px_height</th>\n",
              "      <th>px_width</th>\n",
              "      <th>ram</th>\n",
              "      <th>sc_h</th>\n",
              "      <th>sc_w</th>\n",
              "      <th>talk_time</th>\n",
              "      <th>three_g</th>\n",
              "      <th>touch_screen</th>\n",
              "      <th>wifi</th>\n",
              "      <th>price_range</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>842</td>\n",
              "      <td>0</td>\n",
              "      <td>2.2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>0.6</td>\n",
              "      <td>188</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>20</td>\n",
              "      <td>756</td>\n",
              "      <td>2549</td>\n",
              "      <td>9</td>\n",
              "      <td>7</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1021</td>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>53</td>\n",
              "      <td>0.7</td>\n",
              "      <td>136</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>905</td>\n",
              "      <td>1988</td>\n",
              "      <td>2631</td>\n",
              "      <td>17</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>563</td>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>41</td>\n",
              "      <td>0.9</td>\n",
              "      <td>145</td>\n",
              "      <td>5</td>\n",
              "      <td>...</td>\n",
              "      <td>1263</td>\n",
              "      <td>1716</td>\n",
              "      <td>2603</td>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>615</td>\n",
              "      <td>1</td>\n",
              "      <td>2.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>0.8</td>\n",
              "      <td>131</td>\n",
              "      <td>6</td>\n",
              "      <td>...</td>\n",
              "      <td>1216</td>\n",
              "      <td>1786</td>\n",
              "      <td>2769</td>\n",
              "      <td>16</td>\n",
              "      <td>8</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1821</td>\n",
              "      <td>1</td>\n",
              "      <td>1.2</td>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>44</td>\n",
              "      <td>0.6</td>\n",
              "      <td>141</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>1208</td>\n",
              "      <td>1212</td>\n",
              "      <td>1411</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 21 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ac62f5ce-f85b-404c-a89e-7347c79483ab')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ac62f5ce-f85b-404c-a89e-7347c79483ab button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ac62f5ce-f85b-404c-a89e-7347c79483ab');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iCayLifkHcp",
        "outputId": "a83df1bc-587d-4cdc-c639-661faa061f4d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 21)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lGuNtfKkMCm",
        "outputId": "ad2e28e2-c6d9-4086-9343-07e9004255d9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "battery_power     0\n",
              "bluetooth         0\n",
              "clock_speed       0\n",
              "dual_sim          0\n",
              "front_cam         0\n",
              "4G                0\n",
              "int_memory        0\n",
              "m_dep             0\n",
              "mobile_wt         0\n",
              "n_cores           0\n",
              "primary_camera    0\n",
              "px_height         0\n",
              "px_width          0\n",
              "ram               0\n",
              "sc_h              0\n",
              "sc_w              0\n",
              "talk_time         0\n",
              "three_g           0\n",
              "touch_screen      0\n",
              "wifi              0\n",
              "price_range       0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1RvQIjDkRhx",
        "outputId": "04dadc22-a452-4472-dddb-50fe7e57d883"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2000 entries, 0 to 1999\n",
            "Data columns (total 21 columns):\n",
            " #   Column          Non-Null Count  Dtype  \n",
            "---  ------          --------------  -----  \n",
            " 0   battery_power   2000 non-null   int64  \n",
            " 1   bluetooth       2000 non-null   int64  \n",
            " 2   clock_speed     2000 non-null   float64\n",
            " 3   dual_sim        2000 non-null   int64  \n",
            " 4   front_cam       2000 non-null   int64  \n",
            " 5   4G              2000 non-null   int64  \n",
            " 6   int_memory      2000 non-null   int64  \n",
            " 7   m_dep           2000 non-null   float64\n",
            " 8   mobile_wt       2000 non-null   int64  \n",
            " 9   n_cores         2000 non-null   int64  \n",
            " 10  primary_camera  2000 non-null   int64  \n",
            " 11  px_height       2000 non-null   int64  \n",
            " 12  px_width        2000 non-null   int64  \n",
            " 13  ram             2000 non-null   int64  \n",
            " 14  sc_h            2000 non-null   int64  \n",
            " 15  sc_w            2000 non-null   int64  \n",
            " 16  talk_time       2000 non-null   int64  \n",
            " 17  three_g         2000 non-null   int64  \n",
            " 18  touch_screen    2000 non-null   int64  \n",
            " 19  wifi            2000 non-null   int64  \n",
            " 20  price_range     2000 non-null   int64  \n",
            "dtypes: float64(2), int64(19)\n",
            "memory usage: 328.2 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, LabelEncoder,MinMaxScaler\n",
        "sc=MinMaxScaler()\n",
        "df['battery_power'] = sc.fit_transform(df[['battery_power']])\n",
        "df['mobile_wt'] = sc.fit_transform(df[['mobile_wt']])\n",
        "df['px_height'] = sc.fit_transform(df[['px_height']])\n",
        "df['px_width'] = sc.fit_transform(df[['px_width']])\n",
        "df['ram'] = sc.fit_transform(df[['ram']])\n",
        "df['int_memory'] = sc.fit_transform(df[['int_memory']])\n",
        "df['front_cam'] = sc.fit_transform(df[['front_cam']])\n",
        "df['sc_h'] = sc.fit_transform(df[['sc_h']])\n",
        "df['sc_w'] = sc.fit_transform(df[['sc_w']]) \n",
        "df['talk_time'] = sc.fit_transform(df[['talk_time']])"
      ],
      "metadata": {
        "id": "YY6gLSHvkV-G"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "hWfZ6qtvka8y",
        "outputId": "2dd7e0c1-0197-43f2-8995-6798a1c5a4ca"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   battery_power  bluetooth  clock_speed  dual_sim  front_cam  4G  int_memory  \\\n",
              "0       0.227789          0          2.2         0   0.052632   0    0.080645   \n",
              "1       0.347361          1          0.5         1   0.000000   1    0.822581   \n",
              "2       0.041416          1          0.5         1   0.105263   1    0.629032   \n",
              "3       0.076152          1          2.5         0   0.000000   0    0.129032   \n",
              "4       0.881764          1          1.2         0   0.684211   1    0.677419   \n",
              "\n",
              "   m_dep  mobile_wt  n_cores  ...  px_height  px_width       ram      sc_h  \\\n",
              "0    0.6   0.900000        2  ...   0.010204  0.170895  0.612774  0.285714   \n",
              "1    0.7   0.466667        3  ...   0.461735  0.993324  0.634687  0.857143   \n",
              "2    0.9   0.541667        5  ...   0.644388  0.811749  0.627205  0.428571   \n",
              "3    0.8   0.425000        6  ...   0.620408  0.858478  0.671566  0.785714   \n",
              "4    0.6   0.508333        2  ...   0.616327  0.475300  0.308658  0.214286   \n",
              "\n",
              "       sc_w  talk_time  three_g  touch_screen  wifi  price_range  \n",
              "0  0.388889   0.944444        0             0     1            1  \n",
              "1  0.166667   0.277778        1             1     0            2  \n",
              "2  0.111111   0.388889        1             1     0            2  \n",
              "3  0.444444   0.500000        1             0     0            2  \n",
              "4  0.111111   0.722222        1             1     0            1  \n",
              "\n",
              "[5 rows x 21 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-921e94da-e8bd-4a03-b6f9-9c5abaa9066d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>battery_power</th>\n",
              "      <th>bluetooth</th>\n",
              "      <th>clock_speed</th>\n",
              "      <th>dual_sim</th>\n",
              "      <th>front_cam</th>\n",
              "      <th>4G</th>\n",
              "      <th>int_memory</th>\n",
              "      <th>m_dep</th>\n",
              "      <th>mobile_wt</th>\n",
              "      <th>n_cores</th>\n",
              "      <th>...</th>\n",
              "      <th>px_height</th>\n",
              "      <th>px_width</th>\n",
              "      <th>ram</th>\n",
              "      <th>sc_h</th>\n",
              "      <th>sc_w</th>\n",
              "      <th>talk_time</th>\n",
              "      <th>three_g</th>\n",
              "      <th>touch_screen</th>\n",
              "      <th>wifi</th>\n",
              "      <th>price_range</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.227789</td>\n",
              "      <td>0</td>\n",
              "      <td>2.2</td>\n",
              "      <td>0</td>\n",
              "      <td>0.052632</td>\n",
              "      <td>0</td>\n",
              "      <td>0.080645</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010204</td>\n",
              "      <td>0.170895</td>\n",
              "      <td>0.612774</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.388889</td>\n",
              "      <td>0.944444</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.347361</td>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.822581</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.466667</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>0.461735</td>\n",
              "      <td>0.993324</td>\n",
              "      <td>0.634687</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.277778</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.041416</td>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1</td>\n",
              "      <td>0.105263</td>\n",
              "      <td>1</td>\n",
              "      <td>0.629032</td>\n",
              "      <td>0.9</td>\n",
              "      <td>0.541667</td>\n",
              "      <td>5</td>\n",
              "      <td>...</td>\n",
              "      <td>0.644388</td>\n",
              "      <td>0.811749</td>\n",
              "      <td>0.627205</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.388889</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.076152</td>\n",
              "      <td>1</td>\n",
              "      <td>2.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.129032</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.425000</td>\n",
              "      <td>6</td>\n",
              "      <td>...</td>\n",
              "      <td>0.620408</td>\n",
              "      <td>0.858478</td>\n",
              "      <td>0.671566</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.881764</td>\n",
              "      <td>1</td>\n",
              "      <td>1.2</td>\n",
              "      <td>0</td>\n",
              "      <td>0.684211</td>\n",
              "      <td>1</td>\n",
              "      <td>0.677419</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.508333</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>0.616327</td>\n",
              "      <td>0.475300</td>\n",
              "      <td>0.308658</td>\n",
              "      <td>0.214286</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.722222</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 21 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-921e94da-e8bd-4a03-b6f9-9c5abaa9066d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-921e94da-e8bd-4a03-b6f9-9c5abaa9066d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-921e94da-e8bd-4a03-b6f9-9c5abaa9066d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = df.drop('price_range',axis=1)"
      ],
      "metadata": {
        "id": "YjDIDsmAke03"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label = df[\"price_range\"]"
      ],
      "metadata": {
        "id": "ZgVqIJvxkjja"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.25,random_state = 10)"
      ],
      "metadata": {
        "id": "Wpl_j_1Ckmyp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense"
      ],
      "metadata": {
        "id": "U6RtmVRvksYA"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(10,activation='relu'))\n",
        "\n",
        "model.add(Dense(1,activation='sigmoid'))"
      ],
      "metadata": {
        "id": "jGGA-Nk7kxsl"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "TudmdsLhk18r"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train, validation_split=0.2, epochs=200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tDKyRPyk5Ra",
        "outputId": "4d63e34b-33f6-4398-aef6-0c497c5f46a8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "38/38 [==============================] - 1s 10ms/step - loss: 0.2179 - accuracy: 0.2583 - val_loss: -0.8342 - val_accuracy: 0.2533\n",
            "Epoch 2/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -2.0349 - accuracy: 0.2533 - val_loss: -2.0986 - val_accuracy: 0.2400\n",
            "Epoch 3/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -3.7016 - accuracy: 0.2517 - val_loss: -3.2921 - val_accuracy: 0.2400\n",
            "Epoch 4/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -5.5987 - accuracy: 0.2517 - val_loss: -4.7603 - val_accuracy: 0.2400\n",
            "Epoch 5/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -7.8581 - accuracy: 0.2517 - val_loss: -6.4410 - val_accuracy: 0.2400\n",
            "Epoch 6/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -10.3912 - accuracy: 0.2517 - val_loss: -8.2621 - val_accuracy: 0.2400\n",
            "Epoch 7/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -13.1078 - accuracy: 0.2517 - val_loss: -10.2690 - val_accuracy: 0.2400\n",
            "Epoch 8/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -16.1198 - accuracy: 0.2517 - val_loss: -12.3911 - val_accuracy: 0.2400\n",
            "Epoch 9/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -19.3147 - accuracy: 0.2517 - val_loss: -14.8566 - val_accuracy: 0.2400\n",
            "Epoch 10/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -22.9735 - accuracy: 0.2517 - val_loss: -17.4023 - val_accuracy: 0.2400\n",
            "Epoch 11/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -26.7911 - accuracy: 0.2517 - val_loss: -20.2708 - val_accuracy: 0.2400\n",
            "Epoch 12/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -31.0046 - accuracy: 0.2517 - val_loss: -23.3001 - val_accuracy: 0.2400\n",
            "Epoch 13/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -35.5225 - accuracy: 0.2517 - val_loss: -26.5836 - val_accuracy: 0.2400\n",
            "Epoch 14/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -40.4198 - accuracy: 0.2517 - val_loss: -30.0346 - val_accuracy: 0.2400\n",
            "Epoch 15/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -45.5467 - accuracy: 0.2517 - val_loss: -33.7319 - val_accuracy: 0.2400\n",
            "Epoch 16/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -51.0467 - accuracy: 0.2517 - val_loss: -37.6779 - val_accuracy: 0.2400\n",
            "Epoch 17/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -56.7712 - accuracy: 0.2517 - val_loss: -41.9268 - val_accuracy: 0.2400\n",
            "Epoch 18/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -62.9241 - accuracy: 0.2517 - val_loss: -46.3091 - val_accuracy: 0.2400\n",
            "Epoch 19/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -69.3824 - accuracy: 0.2517 - val_loss: -50.8084 - val_accuracy: 0.2400\n",
            "Epoch 20/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -76.0467 - accuracy: 0.2517 - val_loss: -55.7828 - val_accuracy: 0.2400\n",
            "Epoch 21/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -83.1458 - accuracy: 0.2517 - val_loss: -60.8534 - val_accuracy: 0.2400\n",
            "Epoch 22/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -90.5949 - accuracy: 0.2517 - val_loss: -65.9629 - val_accuracy: 0.2400\n",
            "Epoch 23/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -98.2125 - accuracy: 0.2517 - val_loss: -71.3992 - val_accuracy: 0.2400\n",
            "Epoch 24/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -106.1708 - accuracy: 0.2517 - val_loss: -77.2126 - val_accuracy: 0.2400\n",
            "Epoch 25/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -114.6144 - accuracy: 0.2517 - val_loss: -83.0309 - val_accuracy: 0.2400\n",
            "Epoch 26/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -123.1319 - accuracy: 0.2517 - val_loss: -89.1017 - val_accuracy: 0.2400\n",
            "Epoch 27/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -132.0930 - accuracy: 0.2517 - val_loss: -95.2637 - val_accuracy: 0.2400\n",
            "Epoch 28/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -141.2243 - accuracy: 0.2517 - val_loss: -102.0868 - val_accuracy: 0.2400\n",
            "Epoch 29/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -150.8870 - accuracy: 0.2517 - val_loss: -108.5016 - val_accuracy: 0.2400\n",
            "Epoch 30/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -160.4605 - accuracy: 0.2517 - val_loss: -115.6961 - val_accuracy: 0.2400\n",
            "Epoch 31/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -170.5390 - accuracy: 0.2517 - val_loss: -122.8643 - val_accuracy: 0.2400\n",
            "Epoch 32/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -180.8914 - accuracy: 0.2517 - val_loss: -130.0733 - val_accuracy: 0.2400\n",
            "Epoch 33/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -191.4062 - accuracy: 0.2517 - val_loss: -137.5460 - val_accuracy: 0.2400\n",
            "Epoch 34/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -202.3047 - accuracy: 0.2517 - val_loss: -145.2580 - val_accuracy: 0.2400\n",
            "Epoch 35/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -213.5561 - accuracy: 0.2517 - val_loss: -153.0079 - val_accuracy: 0.2400\n",
            "Epoch 36/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -224.7921 - accuracy: 0.2517 - val_loss: -161.2912 - val_accuracy: 0.2400\n",
            "Epoch 37/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -236.5875 - accuracy: 0.2517 - val_loss: -169.3709 - val_accuracy: 0.2400\n",
            "Epoch 38/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -248.4829 - accuracy: 0.2517 - val_loss: -177.9763 - val_accuracy: 0.2400\n",
            "Epoch 39/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -260.7268 - accuracy: 0.2517 - val_loss: -186.4098 - val_accuracy: 0.2400\n",
            "Epoch 40/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -273.1933 - accuracy: 0.2517 - val_loss: -195.0948 - val_accuracy: 0.2400\n",
            "Epoch 41/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -285.9621 - accuracy: 0.2517 - val_loss: -204.0700 - val_accuracy: 0.2400\n",
            "Epoch 42/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -298.8446 - accuracy: 0.2517 - val_loss: -213.4332 - val_accuracy: 0.2400\n",
            "Epoch 43/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -312.1400 - accuracy: 0.2517 - val_loss: -222.6450 - val_accuracy: 0.2400\n",
            "Epoch 44/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -325.6231 - accuracy: 0.2517 - val_loss: -231.8722 - val_accuracy: 0.2400\n",
            "Epoch 45/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -339.0685 - accuracy: 0.2517 - val_loss: -241.8929 - val_accuracy: 0.2400\n",
            "Epoch 46/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -353.2339 - accuracy: 0.2517 - val_loss: -251.2939 - val_accuracy: 0.2400\n",
            "Epoch 47/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -367.0510 - accuracy: 0.2517 - val_loss: -261.4916 - val_accuracy: 0.2400\n",
            "Epoch 48/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -381.5489 - accuracy: 0.2517 - val_loss: -271.5577 - val_accuracy: 0.2400\n",
            "Epoch 49/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -396.1281 - accuracy: 0.2517 - val_loss: -281.4917 - val_accuracy: 0.2400\n",
            "Epoch 50/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: -410.7447 - accuracy: 0.2517 - val_loss: -292.1340 - val_accuracy: 0.2400\n",
            "Epoch 51/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -425.8756 - accuracy: 0.2517 - val_loss: -302.5840 - val_accuracy: 0.2400\n",
            "Epoch 52/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -441.0648 - accuracy: 0.2517 - val_loss: -313.4285 - val_accuracy: 0.2400\n",
            "Epoch 53/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -456.6736 - accuracy: 0.2517 - val_loss: -324.1305 - val_accuracy: 0.2400\n",
            "Epoch 54/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -472.2051 - accuracy: 0.2517 - val_loss: -335.4539 - val_accuracy: 0.2400\n",
            "Epoch 55/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -488.2599 - accuracy: 0.2517 - val_loss: -346.6427 - val_accuracy: 0.2400\n",
            "Epoch 56/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -504.1732 - accuracy: 0.2517 - val_loss: -357.7742 - val_accuracy: 0.2400\n",
            "Epoch 57/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -520.4576 - accuracy: 0.2517 - val_loss: -369.3454 - val_accuracy: 0.2400\n",
            "Epoch 58/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -537.2617 - accuracy: 0.2517 - val_loss: -380.7454 - val_accuracy: 0.2400\n",
            "Epoch 59/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: -553.9311 - accuracy: 0.2517 - val_loss: -392.2081 - val_accuracy: 0.2400\n",
            "Epoch 60/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -570.6760 - accuracy: 0.2517 - val_loss: -404.5127 - val_accuracy: 0.2400\n",
            "Epoch 61/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -588.0099 - accuracy: 0.2517 - val_loss: -416.8011 - val_accuracy: 0.2400\n",
            "Epoch 62/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -605.6194 - accuracy: 0.2517 - val_loss: -428.7320 - val_accuracy: 0.2400\n",
            "Epoch 63/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -623.0956 - accuracy: 0.2517 - val_loss: -441.3042 - val_accuracy: 0.2400\n",
            "Epoch 64/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -640.8369 - accuracy: 0.2517 - val_loss: -454.0954 - val_accuracy: 0.2400\n",
            "Epoch 65/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -659.0587 - accuracy: 0.2517 - val_loss: -466.5795 - val_accuracy: 0.2400\n",
            "Epoch 66/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -676.8481 - accuracy: 0.2517 - val_loss: -479.5477 - val_accuracy: 0.2400\n",
            "Epoch 67/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -695.3247 - accuracy: 0.2517 - val_loss: -491.9994 - val_accuracy: 0.2400\n",
            "Epoch 68/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -713.8268 - accuracy: 0.2517 - val_loss: -504.8307 - val_accuracy: 0.2400\n",
            "Epoch 69/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -732.6365 - accuracy: 0.2517 - val_loss: -517.9039 - val_accuracy: 0.2400\n",
            "Epoch 70/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -751.7224 - accuracy: 0.2517 - val_loss: -531.1909 - val_accuracy: 0.2400\n",
            "Epoch 71/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -770.9238 - accuracy: 0.2517 - val_loss: -545.0472 - val_accuracy: 0.2400\n",
            "Epoch 72/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -790.4713 - accuracy: 0.2517 - val_loss: -558.7744 - val_accuracy: 0.2400\n",
            "Epoch 73/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -810.0751 - accuracy: 0.2517 - val_loss: -572.6523 - val_accuracy: 0.2400\n",
            "Epoch 74/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -830.1740 - accuracy: 0.2517 - val_loss: -586.4688 - val_accuracy: 0.2400\n",
            "Epoch 75/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -850.0767 - accuracy: 0.2517 - val_loss: -600.5872 - val_accuracy: 0.2400\n",
            "Epoch 76/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -870.5571 - accuracy: 0.2517 - val_loss: -614.3259 - val_accuracy: 0.2400\n",
            "Epoch 77/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -890.8429 - accuracy: 0.2517 - val_loss: -628.8704 - val_accuracy: 0.2400\n",
            "Epoch 78/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -911.5874 - accuracy: 0.2517 - val_loss: -643.5474 - val_accuracy: 0.2400\n",
            "Epoch 79/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -932.3800 - accuracy: 0.2517 - val_loss: -657.5994 - val_accuracy: 0.2400\n",
            "Epoch 80/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -952.8220 - accuracy: 0.2517 - val_loss: -672.7792 - val_accuracy: 0.2400\n",
            "Epoch 81/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -974.0066 - accuracy: 0.2517 - val_loss: -687.2618 - val_accuracy: 0.2400\n",
            "Epoch 82/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -995.2883 - accuracy: 0.2517 - val_loss: -701.8476 - val_accuracy: 0.2400\n",
            "Epoch 83/200\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1016.6340 - accuracy: 0.2517 - val_loss: -717.0621 - val_accuracy: 0.2400\n",
            "Epoch 84/200\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -1038.0431 - accuracy: 0.2517 - val_loss: -732.1617 - val_accuracy: 0.2400\n",
            "Epoch 85/200\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -1059.9177 - accuracy: 0.2517 - val_loss: -747.1902 - val_accuracy: 0.2400\n",
            "Epoch 86/200\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -1081.9009 - accuracy: 0.2517 - val_loss: -762.7450 - val_accuracy: 0.2400\n",
            "Epoch 87/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -1103.7992 - accuracy: 0.2517 - val_loss: -778.5628 - val_accuracy: 0.2400\n",
            "Epoch 88/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -1126.3668 - accuracy: 0.2517 - val_loss: -793.9182 - val_accuracy: 0.2400\n",
            "Epoch 89/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -1148.6150 - accuracy: 0.2517 - val_loss: -809.4459 - val_accuracy: 0.2400\n",
            "Epoch 90/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -1170.9995 - accuracy: 0.2517 - val_loss: -825.4137 - val_accuracy: 0.2400\n",
            "Epoch 91/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -1193.8563 - accuracy: 0.2517 - val_loss: -841.0374 - val_accuracy: 0.2400\n",
            "Epoch 92/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -1216.4878 - accuracy: 0.2517 - val_loss: -857.2224 - val_accuracy: 0.2400\n",
            "Epoch 93/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: -1239.6909 - accuracy: 0.2517 - val_loss: -873.8571 - val_accuracy: 0.2400\n",
            "Epoch 94/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -1263.3231 - accuracy: 0.2517 - val_loss: -889.2584 - val_accuracy: 0.2400\n",
            "Epoch 95/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -1286.4763 - accuracy: 0.2517 - val_loss: -906.2646 - val_accuracy: 0.2400\n",
            "Epoch 96/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -1310.1647 - accuracy: 0.2517 - val_loss: -923.4189 - val_accuracy: 0.2400\n",
            "Epoch 97/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -1334.3396 - accuracy: 0.2517 - val_loss: -939.6818 - val_accuracy: 0.2400\n",
            "Epoch 98/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -1358.1278 - accuracy: 0.2517 - val_loss: -956.5361 - val_accuracy: 0.2400\n",
            "Epoch 99/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -1382.4081 - accuracy: 0.2517 - val_loss: -973.5125 - val_accuracy: 0.2400\n",
            "Epoch 100/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -1406.7285 - accuracy: 0.2517 - val_loss: -990.7078 - val_accuracy: 0.2400\n",
            "Epoch 101/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -1431.2122 - accuracy: 0.2517 - val_loss: -1007.6252 - val_accuracy: 0.2400\n",
            "Epoch 102/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -1455.9940 - accuracy: 0.2517 - val_loss: -1024.9399 - val_accuracy: 0.2400\n",
            "Epoch 103/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -1480.7080 - accuracy: 0.2517 - val_loss: -1042.6127 - val_accuracy: 0.2400\n",
            "Epoch 104/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -1505.8612 - accuracy: 0.2517 - val_loss: -1060.4583 - val_accuracy: 0.2400\n",
            "Epoch 105/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -1531.4202 - accuracy: 0.2517 - val_loss: -1077.5137 - val_accuracy: 0.2400\n",
            "Epoch 106/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -1556.7760 - accuracy: 0.2517 - val_loss: -1095.2985 - val_accuracy: 0.2400\n",
            "Epoch 107/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -1582.2770 - accuracy: 0.2517 - val_loss: -1113.8711 - val_accuracy: 0.2400\n",
            "Epoch 108/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -1608.1803 - accuracy: 0.2517 - val_loss: -1131.7335 - val_accuracy: 0.2400\n",
            "Epoch 109/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -1634.2805 - accuracy: 0.2517 - val_loss: -1149.6581 - val_accuracy: 0.2400\n",
            "Epoch 110/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -1660.4379 - accuracy: 0.2517 - val_loss: -1168.2667 - val_accuracy: 0.2400\n",
            "Epoch 111/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -1686.7889 - accuracy: 0.2517 - val_loss: -1186.9415 - val_accuracy: 0.2400\n",
            "Epoch 112/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -1713.4425 - accuracy: 0.2517 - val_loss: -1205.6525 - val_accuracy: 0.2400\n",
            "Epoch 113/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -1740.2268 - accuracy: 0.2517 - val_loss: -1224.0046 - val_accuracy: 0.2400\n",
            "Epoch 114/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -1766.9198 - accuracy: 0.2517 - val_loss: -1243.0072 - val_accuracy: 0.2400\n",
            "Epoch 115/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -1794.2247 - accuracy: 0.2517 - val_loss: -1261.7958 - val_accuracy: 0.2400\n",
            "Epoch 116/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -1821.6171 - accuracy: 0.2517 - val_loss: -1281.0879 - val_accuracy: 0.2400\n",
            "Epoch 117/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -1849.1616 - accuracy: 0.2517 - val_loss: -1300.2500 - val_accuracy: 0.2400\n",
            "Epoch 118/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -1876.5616 - accuracy: 0.2517 - val_loss: -1319.6530 - val_accuracy: 0.2400\n",
            "Epoch 119/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -1904.3950 - accuracy: 0.2517 - val_loss: -1339.4493 - val_accuracy: 0.2400\n",
            "Epoch 120/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -1932.2040 - accuracy: 0.2517 - val_loss: -1358.8615 - val_accuracy: 0.2400\n",
            "Epoch 121/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -1960.1223 - accuracy: 0.2517 - val_loss: -1378.4271 - val_accuracy: 0.2400\n",
            "Epoch 122/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -1988.4240 - accuracy: 0.2517 - val_loss: -1397.4749 - val_accuracy: 0.2400\n",
            "Epoch 123/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -2016.4529 - accuracy: 0.2517 - val_loss: -1418.5253 - val_accuracy: 0.2400\n",
            "Epoch 124/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -2045.3644 - accuracy: 0.2517 - val_loss: -1437.8503 - val_accuracy: 0.2400\n",
            "Epoch 125/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -2073.6914 - accuracy: 0.2517 - val_loss: -1458.2991 - val_accuracy: 0.2400\n",
            "Epoch 126/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -2102.6987 - accuracy: 0.2517 - val_loss: -1478.6248 - val_accuracy: 0.2400\n",
            "Epoch 127/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -2131.4998 - accuracy: 0.2517 - val_loss: -1498.5770 - val_accuracy: 0.2400\n",
            "Epoch 128/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -2160.6240 - accuracy: 0.2517 - val_loss: -1519.1024 - val_accuracy: 0.2400\n",
            "Epoch 129/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -2190.0291 - accuracy: 0.2517 - val_loss: -1539.7631 - val_accuracy: 0.2400\n",
            "Epoch 130/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -2219.5500 - accuracy: 0.2517 - val_loss: -1560.4368 - val_accuracy: 0.2400\n",
            "Epoch 131/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -2249.1775 - accuracy: 0.2517 - val_loss: -1582.0809 - val_accuracy: 0.2400\n",
            "Epoch 132/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -2279.3306 - accuracy: 0.2517 - val_loss: -1602.8969 - val_accuracy: 0.2400\n",
            "Epoch 133/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -2309.1655 - accuracy: 0.2517 - val_loss: -1623.5369 - val_accuracy: 0.2400\n",
            "Epoch 134/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -2339.2769 - accuracy: 0.2517 - val_loss: -1644.4918 - val_accuracy: 0.2400\n",
            "Epoch 135/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -2369.2666 - accuracy: 0.2517 - val_loss: -1666.5509 - val_accuracy: 0.2400\n",
            "Epoch 136/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -2400.0654 - accuracy: 0.2517 - val_loss: -1687.0005 - val_accuracy: 0.2400\n",
            "Epoch 137/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -2430.6902 - accuracy: 0.2517 - val_loss: -1708.8750 - val_accuracy: 0.2400\n",
            "Epoch 138/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -2461.7939 - accuracy: 0.2517 - val_loss: -1730.3192 - val_accuracy: 0.2400\n",
            "Epoch 139/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -2492.7595 - accuracy: 0.2517 - val_loss: -1752.6176 - val_accuracy: 0.2400\n",
            "Epoch 140/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -2524.4126 - accuracy: 0.2517 - val_loss: -1773.7344 - val_accuracy: 0.2400\n",
            "Epoch 141/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -2555.3877 - accuracy: 0.2517 - val_loss: -1795.9946 - val_accuracy: 0.2400\n",
            "Epoch 142/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -2587.0505 - accuracy: 0.2517 - val_loss: -1817.7866 - val_accuracy: 0.2400\n",
            "Epoch 143/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -2618.4211 - accuracy: 0.2517 - val_loss: -1840.8364 - val_accuracy: 0.2400\n",
            "Epoch 144/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -2650.3760 - accuracy: 0.2517 - val_loss: -1863.1310 - val_accuracy: 0.2400\n",
            "Epoch 145/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -2682.4468 - accuracy: 0.2517 - val_loss: -1884.8995 - val_accuracy: 0.2400\n",
            "Epoch 146/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -2713.9639 - accuracy: 0.2517 - val_loss: -1908.1462 - val_accuracy: 0.2400\n",
            "Epoch 147/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -2746.7429 - accuracy: 0.2517 - val_loss: -1929.6967 - val_accuracy: 0.2400\n",
            "Epoch 148/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -2778.5271 - accuracy: 0.2517 - val_loss: -1952.0698 - val_accuracy: 0.2400\n",
            "Epoch 149/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -2810.5566 - accuracy: 0.2517 - val_loss: -1975.2574 - val_accuracy: 0.2400\n",
            "Epoch 150/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -2843.0315 - accuracy: 0.2517 - val_loss: -1998.0236 - val_accuracy: 0.2400\n",
            "Epoch 151/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -2875.9890 - accuracy: 0.2517 - val_loss: -2020.9252 - val_accuracy: 0.2400\n",
            "Epoch 152/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -2908.6731 - accuracy: 0.2517 - val_loss: -2043.8529 - val_accuracy: 0.2400\n",
            "Epoch 153/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -2941.8518 - accuracy: 0.2517 - val_loss: -2066.7964 - val_accuracy: 0.2400\n",
            "Epoch 154/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -2974.8804 - accuracy: 0.2517 - val_loss: -2090.6023 - val_accuracy: 0.2400\n",
            "Epoch 155/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -3008.5505 - accuracy: 0.2517 - val_loss: -2113.7610 - val_accuracy: 0.2400\n",
            "Epoch 156/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -3042.2773 - accuracy: 0.2517 - val_loss: -2137.9338 - val_accuracy: 0.2400\n",
            "Epoch 157/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -3076.1194 - accuracy: 0.2517 - val_loss: -2161.8728 - val_accuracy: 0.2400\n",
            "Epoch 158/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -3109.9963 - accuracy: 0.2517 - val_loss: -2184.5837 - val_accuracy: 0.2400\n",
            "Epoch 159/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -3143.4324 - accuracy: 0.2517 - val_loss: -2208.8650 - val_accuracy: 0.2400\n",
            "Epoch 160/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -3177.5874 - accuracy: 0.2517 - val_loss: -2232.3660 - val_accuracy: 0.2400\n",
            "Epoch 161/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -3211.4380 - accuracy: 0.2517 - val_loss: -2257.4062 - val_accuracy: 0.2400\n",
            "Epoch 162/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -3246.1541 - accuracy: 0.2517 - val_loss: -2281.3350 - val_accuracy: 0.2400\n",
            "Epoch 163/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: -3280.9187 - accuracy: 0.2517 - val_loss: -2305.0049 - val_accuracy: 0.2400\n",
            "Epoch 164/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -3315.7942 - accuracy: 0.2517 - val_loss: -2329.1128 - val_accuracy: 0.2400\n",
            "Epoch 165/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -3350.7913 - accuracy: 0.2517 - val_loss: -2353.9180 - val_accuracy: 0.2400\n",
            "Epoch 166/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -3386.1831 - accuracy: 0.2517 - val_loss: -2378.1736 - val_accuracy: 0.2400\n",
            "Epoch 167/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -3421.2810 - accuracy: 0.2517 - val_loss: -2403.0996 - val_accuracy: 0.2400\n",
            "Epoch 168/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -3456.7539 - accuracy: 0.2517 - val_loss: -2428.6316 - val_accuracy: 0.2400\n",
            "Epoch 169/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -3492.8118 - accuracy: 0.2517 - val_loss: -2452.9324 - val_accuracy: 0.2400\n",
            "Epoch 170/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -3528.0137 - accuracy: 0.2517 - val_loss: -2479.0120 - val_accuracy: 0.2400\n",
            "Epoch 171/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -3564.3372 - accuracy: 0.2517 - val_loss: -2503.3916 - val_accuracy: 0.2400\n",
            "Epoch 172/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -3600.5671 - accuracy: 0.2517 - val_loss: -2528.9534 - val_accuracy: 0.2400\n",
            "Epoch 173/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -3636.7451 - accuracy: 0.2517 - val_loss: -2554.6309 - val_accuracy: 0.2400\n",
            "Epoch 174/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -3673.1001 - accuracy: 0.2517 - val_loss: -2580.0103 - val_accuracy: 0.2400\n",
            "Epoch 175/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -3709.3245 - accuracy: 0.2517 - val_loss: -2605.7664 - val_accuracy: 0.2400\n",
            "Epoch 176/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -3745.9050 - accuracy: 0.2517 - val_loss: -2631.2600 - val_accuracy: 0.2400\n",
            "Epoch 177/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -3782.4299 - accuracy: 0.2517 - val_loss: -2656.7126 - val_accuracy: 0.2400\n",
            "Epoch 178/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -3819.3928 - accuracy: 0.2517 - val_loss: -2682.2046 - val_accuracy: 0.2400\n",
            "Epoch 179/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -3856.1611 - accuracy: 0.2517 - val_loss: -2708.6794 - val_accuracy: 0.2400\n",
            "Epoch 180/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -3893.8367 - accuracy: 0.2517 - val_loss: -2734.9377 - val_accuracy: 0.2400\n",
            "Epoch 181/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: -3931.0618 - accuracy: 0.2517 - val_loss: -2761.5076 - val_accuracy: 0.2400\n",
            "Epoch 182/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -3969.0591 - accuracy: 0.2517 - val_loss: -2786.6372 - val_accuracy: 0.2400\n",
            "Epoch 183/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: -4006.2300 - accuracy: 0.2517 - val_loss: -2813.8713 - val_accuracy: 0.2400\n",
            "Epoch 184/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -4044.4934 - accuracy: 0.2517 - val_loss: -2840.4326 - val_accuracy: 0.2400\n",
            "Epoch 185/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -4082.4038 - accuracy: 0.2517 - val_loss: -2867.7969 - val_accuracy: 0.2400\n",
            "Epoch 186/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: -4120.7031 - accuracy: 0.2517 - val_loss: -2894.2920 - val_accuracy: 0.2400\n",
            "Epoch 187/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -4158.8652 - accuracy: 0.2517 - val_loss: -2920.3240 - val_accuracy: 0.2400\n",
            "Epoch 188/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -4197.3257 - accuracy: 0.2517 - val_loss: -2947.2454 - val_accuracy: 0.2400\n",
            "Epoch 189/200\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -4235.8594 - accuracy: 0.2517 - val_loss: -2975.6455 - val_accuracy: 0.2400\n",
            "Epoch 190/200\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -4275.5117 - accuracy: 0.2517 - val_loss: -3002.0049 - val_accuracy: 0.2400\n",
            "Epoch 191/200\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -4314.0898 - accuracy: 0.2517 - val_loss: -3029.8318 - val_accuracy: 0.2400\n",
            "Epoch 192/200\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -4353.5073 - accuracy: 0.2517 - val_loss: -3057.2568 - val_accuracy: 0.2400\n",
            "Epoch 193/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -4392.9238 - accuracy: 0.2517 - val_loss: -3084.8115 - val_accuracy: 0.2400\n",
            "Epoch 194/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: -4432.3989 - accuracy: 0.2517 - val_loss: -3112.5693 - val_accuracy: 0.2400\n",
            "Epoch 195/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -4471.9932 - accuracy: 0.2517 - val_loss: -3140.2612 - val_accuracy: 0.2400\n",
            "Epoch 196/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -4512.1802 - accuracy: 0.2517 - val_loss: -3168.7495 - val_accuracy: 0.2400\n",
            "Epoch 197/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -4552.5630 - accuracy: 0.2517 - val_loss: -3197.1072 - val_accuracy: 0.2400\n",
            "Epoch 198/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -4592.8594 - accuracy: 0.2517 - val_loss: -3224.7861 - val_accuracy: 0.2400\n",
            "Epoch 199/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -4632.6616 - accuracy: 0.2517 - val_loss: -3253.4175 - val_accuracy: 0.2400\n",
            "Epoch 200/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -4673.5615 - accuracy: 0.2517 - val_loss: -3281.7239 - val_accuracy: 0.2400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUGNXAnZk910",
        "outputId": "be9cdd44-89ac-4eb9-c7de-80aa2e7e4a08"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16/16 [==============================] - 0s 1ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZaUuMbalC-7",
        "outputId": "2816d7bc-23bc-4ea0-cdce-2914766a171f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred[y_pred>0.5] = 1\n",
        "y_pred[y_pred<=0.5] = 0"
      ],
      "metadata": {
        "id": "j6dtYb7glIsI"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(y_pred,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_oh7liElLg7",
        "outputId": "c45e7d04-bbf7-44e2-b14a-4b13e205fc69"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.252"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "4lG5wvWWlQnR",
        "outputId": "397443af-c5c7-4cf2-8322-1270e21b32c3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from hyperopt import tpe, hp, fmin, STATUS_OK,Trials\n",
        "space = {\n",
        "     \"neurons\": hp.quniform(\"neurons\", 1, 100,1),\n",
        "     \"activation\": hp.choice(\"activation\", ['relu', 'sigmoid', 'tanh']),\n",
        "     \"if_condition\" : hp.uniform(\"if_condition\", 0, 1)\n",
        "}"
      ],
      "metadata": {
        "id": "R-xSZ_-mlVG5"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hyperparameter_tuning(params):\n",
        "  print(params)\n",
        "  model = Sequential()\n",
        "  model.add(Dense(units = params['neurons'],activation=params['activation']))\n",
        "  \n",
        "  if params['if_condition'] < 0.5:\n",
        "    model.add(Dense(32,activation='relu'))\n",
        "\n",
        "\n",
        "  model.add(Dense(1,activation='sigmoid')) \n",
        "\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "  history = model.fit(X_train, y_train,  epochs=100, verbose=0)\n",
        "\n",
        "  scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "  print(f'accuracy : {scores[1]*100}')\n",
        "  return {\"loss\": -scores[1], \"status\": STATUS_OK}\n"
      ],
      "metadata": {
        "id": "O_HG9jS3l10V"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trials = Trials()\n",
        "\n",
        "best = fmin(\n",
        "    fn=hyperparameter_tuning,\n",
        "    space = space, \n",
        "    algo=tpe.suggest, \n",
        "    max_evals=10, \n",
        "    trials=trials\n",
        ")\n",
        "\n",
        "print(\"Best: {}\".format(best))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbDQ-wetl6SK",
        "outputId": "98bb97e0-294d-474e-a99f-296953b76399"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'activation': 'relu', 'if_condition': 0.6909086261301188, 'neurons': 85.0}\n",
            "accuracy : 25.200000405311584\n",
            "{'activation': 'relu', 'if_condition': 0.9703434447175614, 'neurons': 27.0}\n",
            "accuracy : 25.200000405311584\n",
            "{'activation': 'tanh', 'if_condition': 0.8512981192821818, 'neurons': 22.0}\n",
            "accuracy : 37.99999952316284\n",
            "{'activation': 'sigmoid', 'if_condition': 0.2320343917552251, 'neurons': 98.0}\n",
            "accuracy : 25.200000405311584\n",
            "{'activation': 'tanh', 'if_condition': 0.09122166338314819, 'neurons': 84.0}\n",
            "accuracy : 44.200000166893005\n",
            "{'activation': 'sigmoid', 'if_condition': 0.06645137095550135, 'neurons': 98.0}\n",
            "accuracy : 25.200000405311584\n",
            "{'activation': 'relu', 'if_condition': 0.8425159596088583, 'neurons': 29.0}\n",
            "accuracy : 25.200000405311584\n",
            "{'activation': 'tanh', 'if_condition': 0.26574665903515704, 'neurons': 11.0}\n",
            "accuracy : 44.40000057220459\n",
            "{'activation': 'sigmoid', 'if_condition': 0.12263043263120887, 'neurons': 64.0}\n",
            "accuracy : 25.200000405311584\n",
            "{'activation': 'relu', 'if_condition': 0.8556738234879125, 'neurons': 13.0}\n",
            "accuracy : 25.200000405311584\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:20<00:00,  8.08s/it, best loss: -0.4440000057220459]\n",
            "Best: {'activation': 2, 'if_condition': 0.26574665903515704, 'neurons': 11.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(60,activation='tanh'))\n",
        "\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "KDMTJ7VknSf1"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train, validation_split=0.2, epochs=200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8sKW4fxoD9B",
        "outputId": "4048ea51-669c-418b-9eb4-d37be799acdb"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "38/38 [==============================] - 1s 7ms/step - loss: -0.5328 - accuracy: 0.2408 - val_loss: -1.4188 - val_accuracy: 0.2400\n",
            "Epoch 2/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: -3.1164 - accuracy: 0.2517 - val_loss: -2.9423 - val_accuracy: 0.2400\n",
            "Epoch 3/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: -5.2973 - accuracy: 0.2517 - val_loss: -4.3994 - val_accuracy: 0.2400\n",
            "Epoch 4/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: -7.1997 - accuracy: 0.2517 - val_loss: -5.5139 - val_accuracy: 0.2400\n",
            "Epoch 5/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: -8.6792 - accuracy: 0.2517 - val_loss: -6.5037 - val_accuracy: 0.2400\n",
            "Epoch 6/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -10.0721 - accuracy: 0.2517 - val_loss: -7.4099 - val_accuracy: 0.2400\n",
            "Epoch 7/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: -11.4097 - accuracy: 0.2517 - val_loss: -8.2792 - val_accuracy: 0.2400\n",
            "Epoch 8/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: -12.6874 - accuracy: 0.2517 - val_loss: -9.1950 - val_accuracy: 0.2400\n",
            "Epoch 9/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -13.9866 - accuracy: 0.2517 - val_loss: -10.0374 - val_accuracy: 0.2400\n",
            "Epoch 10/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -15.2348 - accuracy: 0.2517 - val_loss: -10.9042 - val_accuracy: 0.2400\n",
            "Epoch 11/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -16.4880 - accuracy: 0.2517 - val_loss: -11.7435 - val_accuracy: 0.2400\n",
            "Epoch 12/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -17.7169 - accuracy: 0.2517 - val_loss: -12.5885 - val_accuracy: 0.2400\n",
            "Epoch 13/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -18.9360 - accuracy: 0.2517 - val_loss: -13.4257 - val_accuracy: 0.2400\n",
            "Epoch 14/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -20.1463 - accuracy: 0.2517 - val_loss: -14.2439 - val_accuracy: 0.2400\n",
            "Epoch 15/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -21.3571 - accuracy: 0.2517 - val_loss: -15.0708 - val_accuracy: 0.2400\n",
            "Epoch 16/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: -22.5574 - accuracy: 0.2517 - val_loss: -15.8889 - val_accuracy: 0.2400\n",
            "Epoch 17/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: -23.7449 - accuracy: 0.2517 - val_loss: -16.7054 - val_accuracy: 0.2400\n",
            "Epoch 18/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -24.9461 - accuracy: 0.2517 - val_loss: -17.5048 - val_accuracy: 0.2400\n",
            "Epoch 19/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -26.1145 - accuracy: 0.2517 - val_loss: -18.3391 - val_accuracy: 0.2400\n",
            "Epoch 20/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: -27.3131 - accuracy: 0.2517 - val_loss: -19.1544 - val_accuracy: 0.2400\n",
            "Epoch 21/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -28.5047 - accuracy: 0.2517 - val_loss: -19.9539 - val_accuracy: 0.2400\n",
            "Epoch 22/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -29.6754 - accuracy: 0.2517 - val_loss: -20.7781 - val_accuracy: 0.2400\n",
            "Epoch 23/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -30.8592 - accuracy: 0.2517 - val_loss: -21.5683 - val_accuracy: 0.2400\n",
            "Epoch 24/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -32.0306 - accuracy: 0.2517 - val_loss: -22.3783 - val_accuracy: 0.2400\n",
            "Epoch 25/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -33.2039 - accuracy: 0.2517 - val_loss: -23.1674 - val_accuracy: 0.2400\n",
            "Epoch 26/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -34.3879 - accuracy: 0.2517 - val_loss: -24.0011 - val_accuracy: 0.2400\n",
            "Epoch 27/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -35.5781 - accuracy: 0.2517 - val_loss: -24.8263 - val_accuracy: 0.2400\n",
            "Epoch 28/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -36.7523 - accuracy: 0.2517 - val_loss: -25.6808 - val_accuracy: 0.2400\n",
            "Epoch 29/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: -37.9630 - accuracy: 0.2517 - val_loss: -26.5107 - val_accuracy: 0.2400\n",
            "Epoch 30/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -39.1467 - accuracy: 0.2517 - val_loss: -27.3727 - val_accuracy: 0.2400\n",
            "Epoch 31/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -40.3584 - accuracy: 0.2517 - val_loss: -28.1929 - val_accuracy: 0.2400\n",
            "Epoch 32/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -41.5401 - accuracy: 0.2517 - val_loss: -29.0625 - val_accuracy: 0.2400\n",
            "Epoch 33/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -42.7385 - accuracy: 0.2517 - val_loss: -29.9022 - val_accuracy: 0.2400\n",
            "Epoch 34/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -43.9479 - accuracy: 0.2517 - val_loss: -30.7239 - val_accuracy: 0.2400\n",
            "Epoch 35/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -45.1542 - accuracy: 0.2517 - val_loss: -31.5914 - val_accuracy: 0.2400\n",
            "Epoch 36/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -46.3426 - accuracy: 0.2517 - val_loss: -32.4690 - val_accuracy: 0.2400\n",
            "Epoch 37/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -47.5591 - accuracy: 0.2517 - val_loss: -33.3098 - val_accuracy: 0.2400\n",
            "Epoch 38/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -48.7572 - accuracy: 0.2517 - val_loss: -34.1534 - val_accuracy: 0.2400\n",
            "Epoch 39/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -49.9743 - accuracy: 0.2517 - val_loss: -35.0567 - val_accuracy: 0.2400\n",
            "Epoch 40/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: -51.1986 - accuracy: 0.2517 - val_loss: -35.9474 - val_accuracy: 0.2400\n",
            "Epoch 41/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -52.4367 - accuracy: 0.2517 - val_loss: -36.8558 - val_accuracy: 0.2400\n",
            "Epoch 42/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -53.6814 - accuracy: 0.2517 - val_loss: -37.8729 - val_accuracy: 0.2400\n",
            "Epoch 43/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -55.0327 - accuracy: 0.2517 - val_loss: -39.0985 - val_accuracy: 0.2400\n",
            "Epoch 44/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -56.4319 - accuracy: 0.2517 - val_loss: -40.2454 - val_accuracy: 0.2400\n",
            "Epoch 45/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: -57.8181 - accuracy: 0.2517 - val_loss: -41.3616 - val_accuracy: 0.2400\n",
            "Epoch 46/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -59.2121 - accuracy: 0.2517 - val_loss: -42.4625 - val_accuracy: 0.2400\n",
            "Epoch 47/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -60.5993 - accuracy: 0.2517 - val_loss: -43.4509 - val_accuracy: 0.2400\n",
            "Epoch 48/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -61.9998 - accuracy: 0.2517 - val_loss: -44.6654 - val_accuracy: 0.2400\n",
            "Epoch 49/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -63.4597 - accuracy: 0.2517 - val_loss: -45.8323 - val_accuracy: 0.2400\n",
            "Epoch 50/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -64.9183 - accuracy: 0.2517 - val_loss: -47.0744 - val_accuracy: 0.2400\n",
            "Epoch 51/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -66.3662 - accuracy: 0.2517 - val_loss: -48.4440 - val_accuracy: 0.2400\n",
            "Epoch 52/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -67.8895 - accuracy: 0.2517 - val_loss: -49.7231 - val_accuracy: 0.2400\n",
            "Epoch 53/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -69.3980 - accuracy: 0.2517 - val_loss: -50.7604 - val_accuracy: 0.2400\n",
            "Epoch 54/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -70.9079 - accuracy: 0.2517 - val_loss: -52.2939 - val_accuracy: 0.2400\n",
            "Epoch 55/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: -72.5310 - accuracy: 0.2517 - val_loss: -53.5943 - val_accuracy: 0.2400\n",
            "Epoch 56/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -74.0619 - accuracy: 0.2517 - val_loss: -55.0286 - val_accuracy: 0.2400\n",
            "Epoch 57/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -75.6607 - accuracy: 0.2517 - val_loss: -56.1792 - val_accuracy: 0.2400\n",
            "Epoch 58/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -77.2820 - accuracy: 0.2517 - val_loss: -57.9971 - val_accuracy: 0.2400\n",
            "Epoch 59/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -79.0561 - accuracy: 0.2517 - val_loss: -59.6142 - val_accuracy: 0.2400\n",
            "Epoch 60/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -80.9123 - accuracy: 0.2517 - val_loss: -61.2496 - val_accuracy: 0.2400\n",
            "Epoch 61/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -82.6675 - accuracy: 0.2517 - val_loss: -63.0073 - val_accuracy: 0.2400\n",
            "Epoch 62/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -84.4688 - accuracy: 0.2517 - val_loss: -64.0919 - val_accuracy: 0.2400\n",
            "Epoch 63/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -86.2279 - accuracy: 0.2517 - val_loss: -66.0755 - val_accuracy: 0.2400\n",
            "Epoch 64/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: -88.0270 - accuracy: 0.2517 - val_loss: -67.8225 - val_accuracy: 0.2400\n",
            "Epoch 65/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: -89.9262 - accuracy: 0.2517 - val_loss: -69.3130 - val_accuracy: 0.2400\n",
            "Epoch 66/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -91.8677 - accuracy: 0.2517 - val_loss: -71.5729 - val_accuracy: 0.2400\n",
            "Epoch 67/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: -93.7502 - accuracy: 0.2517 - val_loss: -73.3485 - val_accuracy: 0.2400\n",
            "Epoch 68/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -95.8052 - accuracy: 0.2517 - val_loss: -75.0808 - val_accuracy: 0.2400\n",
            "Epoch 69/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -97.8213 - accuracy: 0.2517 - val_loss: -77.2565 - val_accuracy: 0.2400\n",
            "Epoch 70/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -99.8747 - accuracy: 0.2517 - val_loss: -78.4521 - val_accuracy: 0.2400\n",
            "Epoch 71/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -101.9168 - accuracy: 0.2517 - val_loss: -80.8969 - val_accuracy: 0.2400\n",
            "Epoch 72/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -103.9917 - accuracy: 0.2517 - val_loss: -82.6855 - val_accuracy: 0.2400\n",
            "Epoch 73/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: -105.9513 - accuracy: 0.2517 - val_loss: -84.2938 - val_accuracy: 0.2400\n",
            "Epoch 74/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -107.8475 - accuracy: 0.2517 - val_loss: -86.5717 - val_accuracy: 0.2400\n",
            "Epoch 75/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -110.0295 - accuracy: 0.2517 - val_loss: -88.5563 - val_accuracy: 0.2400\n",
            "Epoch 76/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -112.0649 - accuracy: 0.2517 - val_loss: -90.2627 - val_accuracy: 0.2400\n",
            "Epoch 77/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -114.1297 - accuracy: 0.2517 - val_loss: -92.1398 - val_accuracy: 0.2400\n",
            "Epoch 78/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -116.2664 - accuracy: 0.2517 - val_loss: -94.5510 - val_accuracy: 0.2400\n",
            "Epoch 79/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: -118.6384 - accuracy: 0.2517 - val_loss: -96.9744 - val_accuracy: 0.2400\n",
            "Epoch 80/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -121.0220 - accuracy: 0.2517 - val_loss: -99.2518 - val_accuracy: 0.2400\n",
            "Epoch 81/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: -123.3255 - accuracy: 0.2517 - val_loss: -101.8324 - val_accuracy: 0.2400\n",
            "Epoch 82/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -125.8207 - accuracy: 0.2525 - val_loss: -104.3303 - val_accuracy: 0.2433\n",
            "Epoch 83/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: -128.2337 - accuracy: 0.2525 - val_loss: -106.8764 - val_accuracy: 0.2467\n",
            "Epoch 84/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: -130.5809 - accuracy: 0.2575 - val_loss: -108.4419 - val_accuracy: 0.2467\n",
            "Epoch 85/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -132.9506 - accuracy: 0.2592 - val_loss: -110.9581 - val_accuracy: 0.2467\n",
            "Epoch 86/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: -135.2972 - accuracy: 0.2608 - val_loss: -113.5875 - val_accuracy: 0.2833\n",
            "Epoch 87/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -137.5258 - accuracy: 0.2708 - val_loss: -115.1829 - val_accuracy: 0.2600\n",
            "Epoch 88/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: -139.8662 - accuracy: 0.2725 - val_loss: -117.1045 - val_accuracy: 0.2600\n",
            "Epoch 89/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -142.0854 - accuracy: 0.2767 - val_loss: -119.3758 - val_accuracy: 0.2833\n",
            "Epoch 90/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -144.2142 - accuracy: 0.2767 - val_loss: -121.4357 - val_accuracy: 0.2867\n",
            "Epoch 91/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -146.4964 - accuracy: 0.2817 - val_loss: -123.1540 - val_accuracy: 0.2867\n",
            "Epoch 92/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -148.5270 - accuracy: 0.2875 - val_loss: -124.9071 - val_accuracy: 0.2900\n",
            "Epoch 93/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -150.7660 - accuracy: 0.2817 - val_loss: -127.3385 - val_accuracy: 0.3033\n",
            "Epoch 94/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -152.9689 - accuracy: 0.2908 - val_loss: -129.1033 - val_accuracy: 0.3033\n",
            "Epoch 95/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -155.1127 - accuracy: 0.3025 - val_loss: -130.7875 - val_accuracy: 0.3000\n",
            "Epoch 96/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: -157.1360 - accuracy: 0.3017 - val_loss: -132.6985 - val_accuracy: 0.3033\n",
            "Epoch 97/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -159.3501 - accuracy: 0.3067 - val_loss: -134.6869 - val_accuracy: 0.3067\n",
            "Epoch 98/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -161.3326 - accuracy: 0.3192 - val_loss: -136.0669 - val_accuracy: 0.3033\n",
            "Epoch 99/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -163.3282 - accuracy: 0.3042 - val_loss: -138.6633 - val_accuracy: 0.3300\n",
            "Epoch 100/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -165.6167 - accuracy: 0.3150 - val_loss: -140.5089 - val_accuracy: 0.3333\n",
            "Epoch 101/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -167.6588 - accuracy: 0.3133 - val_loss: -142.4339 - val_accuracy: 0.3600\n",
            "Epoch 102/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: -169.6310 - accuracy: 0.3233 - val_loss: -143.9829 - val_accuracy: 0.3333\n",
            "Epoch 103/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -171.7923 - accuracy: 0.3325 - val_loss: -145.9309 - val_accuracy: 0.3500\n",
            "Epoch 104/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: -173.8264 - accuracy: 0.3292 - val_loss: -147.1530 - val_accuracy: 0.3300\n",
            "Epoch 105/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -175.8155 - accuracy: 0.3300 - val_loss: -149.0406 - val_accuracy: 0.3300\n",
            "Epoch 106/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -177.8977 - accuracy: 0.3375 - val_loss: -150.7346 - val_accuracy: 0.3300\n",
            "Epoch 107/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: -179.9569 - accuracy: 0.3283 - val_loss: -153.0137 - val_accuracy: 0.3667\n",
            "Epoch 108/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -181.7860 - accuracy: 0.3425 - val_loss: -154.6957 - val_accuracy: 0.3667\n",
            "Epoch 109/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -183.8919 - accuracy: 0.3400 - val_loss: -156.5096 - val_accuracy: 0.3667\n",
            "Epoch 110/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -185.9484 - accuracy: 0.3525 - val_loss: -157.4442 - val_accuracy: 0.3300\n",
            "Epoch 111/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -187.9385 - accuracy: 0.3458 - val_loss: -159.8990 - val_accuracy: 0.3733\n",
            "Epoch 112/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: -190.0003 - accuracy: 0.3483 - val_loss: -161.6681 - val_accuracy: 0.3767\n",
            "Epoch 113/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: -191.9449 - accuracy: 0.3508 - val_loss: -163.4235 - val_accuracy: 0.3800\n",
            "Epoch 114/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -193.9321 - accuracy: 0.3633 - val_loss: -164.8725 - val_accuracy: 0.3733\n",
            "Epoch 115/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -195.9933 - accuracy: 0.3525 - val_loss: -166.9655 - val_accuracy: 0.3867\n",
            "Epoch 116/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -197.9606 - accuracy: 0.3700 - val_loss: -168.2903 - val_accuracy: 0.3767\n",
            "Epoch 117/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -199.9026 - accuracy: 0.3667 - val_loss: -169.7430 - val_accuracy: 0.3633\n",
            "Epoch 118/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -201.8434 - accuracy: 0.3667 - val_loss: -171.6266 - val_accuracy: 0.3800\n",
            "Epoch 119/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: -203.8271 - accuracy: 0.3667 - val_loss: -173.6077 - val_accuracy: 0.3900\n",
            "Epoch 120/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -205.7954 - accuracy: 0.3717 - val_loss: -175.3698 - val_accuracy: 0.3933\n",
            "Epoch 121/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -207.7317 - accuracy: 0.3725 - val_loss: -176.6236 - val_accuracy: 0.3800\n",
            "Epoch 122/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -209.7136 - accuracy: 0.3808 - val_loss: -178.2794 - val_accuracy: 0.3800\n",
            "Epoch 123/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: -211.6911 - accuracy: 0.3708 - val_loss: -180.5966 - val_accuracy: 0.4167\n",
            "Epoch 124/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -213.6416 - accuracy: 0.3908 - val_loss: -180.8635 - val_accuracy: 0.3600\n",
            "Epoch 125/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: -215.4088 - accuracy: 0.3750 - val_loss: -183.8803 - val_accuracy: 0.4133\n",
            "Epoch 126/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -217.4668 - accuracy: 0.3842 - val_loss: -185.3166 - val_accuracy: 0.4100\n",
            "Epoch 127/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: -219.4158 - accuracy: 0.3867 - val_loss: -187.0096 - val_accuracy: 0.4133\n",
            "Epoch 128/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: -221.3204 - accuracy: 0.3858 - val_loss: -188.5596 - val_accuracy: 0.4133\n",
            "Epoch 129/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -223.1890 - accuracy: 0.3892 - val_loss: -190.5290 - val_accuracy: 0.4233\n",
            "Epoch 130/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -225.0949 - accuracy: 0.3900 - val_loss: -191.2655 - val_accuracy: 0.3867\n",
            "Epoch 131/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -227.1417 - accuracy: 0.3917 - val_loss: -193.7540 - val_accuracy: 0.4200\n",
            "Epoch 132/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -228.9745 - accuracy: 0.3992 - val_loss: -195.2249 - val_accuracy: 0.4200\n",
            "Epoch 133/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -230.8962 - accuracy: 0.3992 - val_loss: -196.6454 - val_accuracy: 0.4133\n",
            "Epoch 134/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -232.8134 - accuracy: 0.3967 - val_loss: -198.4632 - val_accuracy: 0.4200\n",
            "Epoch 135/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: -234.6861 - accuracy: 0.4008 - val_loss: -200.0397 - val_accuracy: 0.4200\n",
            "Epoch 136/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -236.5146 - accuracy: 0.4050 - val_loss: -201.6460 - val_accuracy: 0.4200\n",
            "Epoch 137/200\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -238.4704 - accuracy: 0.3992 - val_loss: -203.5095 - val_accuracy: 0.4233\n",
            "Epoch 138/200\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -240.3847 - accuracy: 0.4017 - val_loss: -205.0536 - val_accuracy: 0.4200\n",
            "Epoch 139/200\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -242.1677 - accuracy: 0.4017 - val_loss: -207.1777 - val_accuracy: 0.4367\n",
            "Epoch 140/200\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -244.1277 - accuracy: 0.4125 - val_loss: -208.0408 - val_accuracy: 0.4200\n",
            "Epoch 141/200\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -246.0683 - accuracy: 0.4075 - val_loss: -209.9488 - val_accuracy: 0.4267\n",
            "Epoch 142/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -247.9257 - accuracy: 0.4042 - val_loss: -211.8493 - val_accuracy: 0.4333\n",
            "Epoch 143/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -249.7725 - accuracy: 0.4150 - val_loss: -213.1227 - val_accuracy: 0.4233\n",
            "Epoch 144/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -251.6550 - accuracy: 0.4100 - val_loss: -215.1150 - val_accuracy: 0.4333\n",
            "Epoch 145/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -253.5171 - accuracy: 0.4083 - val_loss: -216.8108 - val_accuracy: 0.4333\n",
            "Epoch 146/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: -255.4111 - accuracy: 0.4150 - val_loss: -217.9057 - val_accuracy: 0.4267\n",
            "Epoch 147/200\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -257.2345 - accuracy: 0.4150 - val_loss: -219.6353 - val_accuracy: 0.4300\n",
            "Epoch 148/200\n",
            "38/38 [==============================] - 0s 8ms/step - loss: -259.1114 - accuracy: 0.4167 - val_loss: -220.7821 - val_accuracy: 0.4200\n",
            "Epoch 149/200\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -261.0063 - accuracy: 0.4150 - val_loss: -222.9872 - val_accuracy: 0.4300\n",
            "Epoch 150/200\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -262.8304 - accuracy: 0.4125 - val_loss: -224.9633 - val_accuracy: 0.4333\n",
            "Epoch 151/200\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -264.6457 - accuracy: 0.4208 - val_loss: -226.3453 - val_accuracy: 0.4333\n",
            "Epoch 152/200\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -266.5839 - accuracy: 0.4142 - val_loss: -228.1396 - val_accuracy: 0.4333\n",
            "Epoch 153/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -268.3253 - accuracy: 0.4233 - val_loss: -228.8056 - val_accuracy: 0.4267\n",
            "Epoch 154/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -270.2036 - accuracy: 0.4175 - val_loss: -230.7008 - val_accuracy: 0.4300\n",
            "Epoch 155/200\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -272.0658 - accuracy: 0.4217 - val_loss: -232.5308 - val_accuracy: 0.4333\n",
            "Epoch 156/200\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -273.9405 - accuracy: 0.4225 - val_loss: -234.2490 - val_accuracy: 0.4333\n",
            "Epoch 157/200\n",
            "38/38 [==============================] - 0s 4ms/step - loss: -275.7285 - accuracy: 0.4217 - val_loss: -235.9972 - val_accuracy: 0.4300\n",
            "Epoch 158/200\n",
            "38/38 [==============================] - 0s 7ms/step - loss: -277.5851 - accuracy: 0.4208 - val_loss: -237.7701 - val_accuracy: 0.4367\n",
            "Epoch 159/200\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -279.4562 - accuracy: 0.4233 - val_loss: -238.9210 - val_accuracy: 0.4333\n",
            "Epoch 160/200\n",
            "38/38 [==============================] - 0s 5ms/step - loss: -281.3083 - accuracy: 0.4250 - val_loss: -240.5642 - val_accuracy: 0.4333\n",
            "Epoch 161/200\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -283.1746 - accuracy: 0.4292 - val_loss: -242.3355 - val_accuracy: 0.4300\n",
            "Epoch 162/200\n",
            "38/38 [==============================] - 0s 6ms/step - loss: -284.9555 - accuracy: 0.4233 - val_loss: -244.1710 - val_accuracy: 0.4367\n",
            "Epoch 163/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -286.8188 - accuracy: 0.4275 - val_loss: -245.3633 - val_accuracy: 0.4300\n",
            "Epoch 164/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: -288.5739 - accuracy: 0.4250 - val_loss: -247.3613 - val_accuracy: 0.4367\n",
            "Epoch 165/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: -290.4684 - accuracy: 0.4308 - val_loss: -248.0604 - val_accuracy: 0.4333\n",
            "Epoch 166/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -292.2488 - accuracy: 0.4275 - val_loss: -250.3167 - val_accuracy: 0.4300\n",
            "Epoch 167/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -294.1302 - accuracy: 0.4292 - val_loss: -251.9339 - val_accuracy: 0.4367\n",
            "Epoch 168/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -295.9560 - accuracy: 0.4258 - val_loss: -253.6789 - val_accuracy: 0.4433\n",
            "Epoch 169/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: -297.7772 - accuracy: 0.4308 - val_loss: -255.0863 - val_accuracy: 0.4367\n",
            "Epoch 170/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -299.4645 - accuracy: 0.4300 - val_loss: -257.1082 - val_accuracy: 0.4567\n",
            "Epoch 171/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -301.4073 - accuracy: 0.4308 - val_loss: -257.5335 - val_accuracy: 0.4300\n",
            "Epoch 172/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -303.2464 - accuracy: 0.4325 - val_loss: -260.1116 - val_accuracy: 0.4500\n",
            "Epoch 173/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -305.0066 - accuracy: 0.4308 - val_loss: -261.4711 - val_accuracy: 0.4433\n",
            "Epoch 174/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: -306.8106 - accuracy: 0.4333 - val_loss: -263.2656 - val_accuracy: 0.4533\n",
            "Epoch 175/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -308.6774 - accuracy: 0.4342 - val_loss: -264.0070 - val_accuracy: 0.4300\n",
            "Epoch 176/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: -310.4410 - accuracy: 0.4342 - val_loss: -265.9008 - val_accuracy: 0.4367\n",
            "Epoch 177/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -312.3291 - accuracy: 0.4350 - val_loss: -267.6157 - val_accuracy: 0.4433\n",
            "Epoch 178/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -314.1357 - accuracy: 0.4333 - val_loss: -269.3196 - val_accuracy: 0.4500\n",
            "Epoch 179/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: -315.9507 - accuracy: 0.4358 - val_loss: -270.5841 - val_accuracy: 0.4400\n",
            "Epoch 180/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -317.7776 - accuracy: 0.4367 - val_loss: -272.4308 - val_accuracy: 0.4500\n",
            "Epoch 181/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -319.5690 - accuracy: 0.4350 - val_loss: -274.1416 - val_accuracy: 0.4567\n",
            "Epoch 182/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -321.3790 - accuracy: 0.4392 - val_loss: -275.4992 - val_accuracy: 0.4500\n",
            "Epoch 183/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -323.2258 - accuracy: 0.4408 - val_loss: -277.1559 - val_accuracy: 0.4567\n",
            "Epoch 184/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -324.9902 - accuracy: 0.4392 - val_loss: -278.6715 - val_accuracy: 0.4567\n",
            "Epoch 185/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -326.8692 - accuracy: 0.4342 - val_loss: -280.4103 - val_accuracy: 0.4600\n",
            "Epoch 186/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: -328.6637 - accuracy: 0.4450 - val_loss: -281.2134 - val_accuracy: 0.4400\n",
            "Epoch 187/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -330.4362 - accuracy: 0.4358 - val_loss: -283.3140 - val_accuracy: 0.4567\n",
            "Epoch 188/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -332.2861 - accuracy: 0.4417 - val_loss: -284.2255 - val_accuracy: 0.4400\n",
            "Epoch 189/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: -334.0083 - accuracy: 0.4425 - val_loss: -286.4774 - val_accuracy: 0.4567\n",
            "Epoch 190/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -335.8714 - accuracy: 0.4375 - val_loss: -288.3287 - val_accuracy: 0.4633\n",
            "Epoch 191/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: -337.6444 - accuracy: 0.4442 - val_loss: -289.6961 - val_accuracy: 0.4600\n",
            "Epoch 192/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -339.5027 - accuracy: 0.4417 - val_loss: -290.7197 - val_accuracy: 0.4533\n",
            "Epoch 193/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: -341.3110 - accuracy: 0.4442 - val_loss: -292.3177 - val_accuracy: 0.4533\n",
            "Epoch 194/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: -343.0760 - accuracy: 0.4417 - val_loss: -293.9572 - val_accuracy: 0.4533\n",
            "Epoch 195/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -344.8689 - accuracy: 0.4433 - val_loss: -295.7480 - val_accuracy: 0.4567\n",
            "Epoch 196/200\n",
            "38/38 [==============================] - 0s 3ms/step - loss: -346.7365 - accuracy: 0.4442 - val_loss: -297.4663 - val_accuracy: 0.4633\n",
            "Epoch 197/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -348.5760 - accuracy: 0.4417 - val_loss: -299.2597 - val_accuracy: 0.4633\n",
            "Epoch 198/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -350.3008 - accuracy: 0.4450 - val_loss: -300.4524 - val_accuracy: 0.4600\n",
            "Epoch 199/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -352.1900 - accuracy: 0.4475 - val_loss: -302.2008 - val_accuracy: 0.4633\n",
            "Epoch 200/200\n",
            "38/38 [==============================] - 0s 2ms/step - loss: -353.8928 - accuracy: 0.4433 - val_loss: -303.9567 - val_accuracy: 0.4667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTZgYgkYoSs3",
        "outputId": "b8bd888d-d10b-492a-b629-84e626f8cc33"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16/16 [==============================] - 0s 1ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred[y_pred>0.3670503415962597] = 1\n",
        "y_pred[y_pred<=0.3670503415962597] = 0"
      ],
      "metadata": {
        "id": "Y1JMOVPUoYRB"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(y_pred,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXRoovxGolYU",
        "outputId": "1172ca1a-f107-4ffb-bf5d-fbb542fa8fd2"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.434"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    }
  ]
}